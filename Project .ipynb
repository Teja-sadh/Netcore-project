{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Embedding, Activation, Flatten,Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('dataset.xlsx')\n",
    "sentences = []\n",
    "labels = []\n",
    "for row in range(len(data)):\n",
    "    sentences.append(str(data['Subject'][row]))\n",
    "    labels.append(str(data['Category'][row]))\n",
    "labels = [cat.lower() for cat in labels]\n",
    "sentences = [word.lower() for word in sentences]\n",
    "X = np.array(sentences)\n",
    "Y = np.array(labels)\n",
    "X, Y = shuffle(X,Y, random_state=0)\n",
    "X_train = X[0:12000]\n",
    "X_test = X[12000:14814]\n",
    "\n",
    "#reshape Y\n",
    "Y = Y.reshape(Y.shape[0],1)\n",
    "Y_train = Y[0:12000,:]\n",
    "Y_test = Y[12000:14814,:]\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n0123456789',oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train,padding='post')\n",
    "vocab_size = len(word_index)+1\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test,padding='post',maxlen=len(X_train[1]))\n",
    "label_encoder  = LabelEncoder()\n",
    "Y_train = label_encoder.fit_transform(Y_train)\n",
    "Y_train = to_categorical(Y_train,num_classes=5)\n",
    "Y_test = label_encoder.fit_transform(Y_test)\n",
    "Y_test = to_categorical(Y_test,num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tokenizer_category.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_model(X_train):\n",
    "    max_len = len(X_train[0])\n",
    "    embedding_dim = 16\n",
    "    vocab_size = len(word_index) + 1 \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,embedding_dim,input_length=max_len))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    #model.add(Flatten())\n",
    "    model.add(Dense(16,activation = 'relu'))\n",
    "    model.add(Dense(5,activation='softmax'))\n",
    "    #model.add(Dense(max_len//2,activation = 'sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 24, 16)            42208     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 42,565\n",
      "Trainable params: 42,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 12000 samples, validate on 2814 samples\n",
      "Epoch 1/15\n",
      "12000/12000 [==============================] - 2s 153us/sample - loss: 1.3383 - accuracy: 0.4719 - val_loss: 1.0821 - val_accuracy: 0.5721\n",
      "Epoch 2/15\n",
      "12000/12000 [==============================] - 1s 82us/sample - loss: 0.8031 - accuracy: 0.7359 - val_loss: 0.6179 - val_accuracy: 0.8092\n",
      "Epoch 3/15\n",
      "12000/12000 [==============================] - 1s 67us/sample - loss: 0.5075 - accuracy: 0.8435 - val_loss: 0.4506 - val_accuracy: 0.8582\n",
      "Epoch 4/15\n",
      "12000/12000 [==============================] - 1s 54us/sample - loss: 0.3870 - accuracy: 0.8795 - val_loss: 0.3847 - val_accuracy: 0.8685\n",
      "Epoch 5/15\n",
      "12000/12000 [==============================] - 1s 59us/sample - loss: 0.3244 - accuracy: 0.8969 - val_loss: 0.3368 - val_accuracy: 0.8898\n",
      "Epoch 6/15\n",
      "12000/12000 [==============================] - 1s 54us/sample - loss: 0.2819 - accuracy: 0.9075 - val_loss: 0.3061 - val_accuracy: 0.8987\n",
      "Epoch 7/15\n",
      "12000/12000 [==============================] - 1s 55us/sample - loss: 0.2528 - accuracy: 0.9168 - val_loss: 0.2848 - val_accuracy: 0.9023\n",
      "Epoch 8/15\n",
      "12000/12000 [==============================] - 1s 56us/sample - loss: 0.2300 - accuracy: 0.9241 - val_loss: 0.2696 - val_accuracy: 0.9062\n",
      "Epoch 9/15\n",
      "12000/12000 [==============================] - 1s 54us/sample - loss: 0.2124 - accuracy: 0.9301 - val_loss: 0.2577 - val_accuracy: 0.9122\n",
      "Epoch 10/15\n",
      "12000/12000 [==============================] - 1s 56us/sample - loss: 0.1978 - accuracy: 0.9340 - val_loss: 0.2461 - val_accuracy: 0.9190\n",
      "Epoch 11/15\n",
      "12000/12000 [==============================] - 1s 54us/sample - loss: 0.1852 - accuracy: 0.9374 - val_loss: 0.2409 - val_accuracy: 0.9247\n",
      "Epoch 12/15\n",
      "12000/12000 [==============================] - 1s 56us/sample - loss: 0.1755 - accuracy: 0.9403 - val_loss: 0.2344 - val_accuracy: 0.9261\n",
      "Epoch 13/15\n",
      "12000/12000 [==============================] - 1s 55us/sample - loss: 0.1671 - accuracy: 0.9416 - val_loss: 0.2304 - val_accuracy: 0.9282\n",
      "Epoch 14/15\n",
      "12000/12000 [==============================] - 1s 56us/sample - loss: 0.1585 - accuracy: 0.9439 - val_loss: 0.2300 - val_accuracy: 0.9286\n",
      "Epoch 15/15\n",
      "12000/12000 [==============================] - 1s 69us/sample - loss: 0.1529 - accuracy: 0.9438 - val_loss: 0.2250 - val_accuracy: 0.9293\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "model = keras_model(X_train)\n",
    "history=model.fit(X_train,Y_train,epochs=num_epochs,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('category_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {0:\"Active Again\",1:\"Easy to Bank\",2:\"Onboarding\",3:\"Regulatory & Mandatory\",4:\"Take More\"}\n",
    "products = data['Product']\n",
    "products = [i.lower() for i in products]\n",
    "X_prod = np.array(sentences)\n",
    "Y_prod = np.array(products)\n",
    "X_prod, Y_prod = shuffle(X_prod,Y_prod, random_state=0)\n",
    "X_train_prod = X_prod[0:12000]\n",
    "X_test_prod = X_prod[12000:14814]\n",
    "\n",
    "#reshape Y\n",
    "Y_prod = Y_prod.reshape(Y_prod.shape[0],1)\n",
    "Y_train_prod = Y_prod[0:12000,:]\n",
    "Y_test_prod = Y_prod[12000:14814,:]\n",
    "tokenizer_k = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n0123456789',oov_token=\"<OOV>\")\n",
    "tokenizer_k.fit_on_texts(X_train_prod)\n",
    "word_index_k = tokenizer_k.word_index\n",
    "X_train_prod = tokenizer_k.texts_to_sequences(X_train_prod)\n",
    "X_train_prod = pad_sequences(X_train_prod,padding='post')\n",
    "vocab_size_k = len(word_index_k)+1\n",
    "\n",
    "X_test_prod = tokenizer_k.texts_to_sequences(X_test_prod)\n",
    "X_test_prod = pad_sequences(X_test_prod,padding='post',maxlen=len(X_train_prod[1]))\n",
    "label_encoder  = LabelEncoder()\n",
    "Y_train_prod = label_encoder.fit_transform(Y_train_prod)\n",
    "Y_train_prod = to_categorical(Y_train_prod,num_classes=8)\n",
    "Y_test_prod = label_encoder.fit_transform(Y_test_prod)\n",
    "Y_test_prod = to_categorical(Y_test_prod,num_classes=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tokenizer_product.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer_k, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_prod_model(X_train_prod,vocab_size):\n",
    "    max_len = len(X_train_prod[0])\n",
    "    embedding_dim = 16\n",
    "    #vocab_size = len(word_index_k) + 1 \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,embedding_dim,input_length=max_len))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    #model.add(Flatten())\n",
    "    model.add(Dense(16,activation = 'relu'))\n",
    "    model.add(Dense(8,activation='softmax'))\n",
    "    #model.add(Dense(max_len//2,activation = 'sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 24, 16)            42208     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 42,616\n",
      "Trainable params: 42,616\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 12000 samples, validate on 2814 samples\n",
      "Epoch 1/15\n",
      "12000/12000 [==============================] - 1s 98us/sample - loss: 1.4902 - accuracy: 0.5634 - val_loss: 1.0865 - val_accuracy: 0.7040\n",
      "Epoch 2/15\n",
      "12000/12000 [==============================] - 1s 65us/sample - loss: 0.8354 - accuracy: 0.7285 - val_loss: 0.5803 - val_accuracy: 0.8191\n",
      "Epoch 3/15\n",
      "12000/12000 [==============================] - 1s 61us/sample - loss: 0.4913 - accuracy: 0.8612 - val_loss: 0.3858 - val_accuracy: 0.8913\n",
      "Epoch 4/15\n",
      "12000/12000 [==============================] - 1s 65us/sample - loss: 0.3428 - accuracy: 0.9067 - val_loss: 0.2922 - val_accuracy: 0.9136\n",
      "Epoch 5/15\n",
      "12000/12000 [==============================] - 1s 67us/sample - loss: 0.2598 - accuracy: 0.9318 - val_loss: 0.2384 - val_accuracy: 0.9332\n",
      "Epoch 6/15\n",
      "12000/12000 [==============================] - 1s 54us/sample - loss: 0.2070 - accuracy: 0.9465 - val_loss: 0.2053 - val_accuracy: 0.9424\n",
      "Epoch 7/15\n",
      "12000/12000 [==============================] - ETA: 0s - loss: 0.1715 - accuracy: 0.95 - 1s 54us/sample - loss: 0.1709 - accuracy: 0.9542 - val_loss: 0.1850 - val_accuracy: 0.9478\n",
      "Epoch 8/15\n",
      "12000/12000 [==============================] - 1s 54us/sample - loss: 0.1459 - accuracy: 0.9587 - val_loss: 0.1675 - val_accuracy: 0.9545\n",
      "Epoch 9/15\n",
      "12000/12000 [==============================] - 1s 60us/sample - loss: 0.1268 - accuracy: 0.9643 - val_loss: 0.1560 - val_accuracy: 0.9570\n",
      "Epoch 10/15\n",
      "12000/12000 [==============================] - 1s 59us/sample - loss: 0.1122 - accuracy: 0.9690 - val_loss: 0.1459 - val_accuracy: 0.9591\n",
      "Epoch 11/15\n",
      "12000/12000 [==============================] - 1s 64us/sample - loss: 0.1009 - accuracy: 0.9706 - val_loss: 0.1408 - val_accuracy: 0.9620ss: 0.1025 - accuracy: 0.\n",
      "Epoch 12/15\n",
      "12000/12000 [==============================] - 1s 52us/sample - loss: 0.0912 - accuracy: 0.9735 - val_loss: 0.1355 - val_accuracy: 0.9641\n",
      "Epoch 13/15\n",
      "12000/12000 [==============================] - 1s 58us/sample - loss: 0.0835 - accuracy: 0.9767 - val_loss: 0.1300 - val_accuracy: 0.9620\n",
      "Epoch 14/15\n",
      "12000/12000 [==============================] - 1s 62us/sample - loss: 0.0770 - accuracy: 0.9772 - val_loss: 0.1279 - val_accuracy: 0.9623\n",
      "Epoch 15/15\n",
      "12000/12000 [==============================] - 1s 57us/sample - loss: 0.0720 - accuracy: 0.9792 - val_loss: 0.1251 - val_accuracy: 0.9662\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "prod_model = keras_prod_model(X_train_prod,vocab_size_k)\n",
    "prod_history=prod_model.fit(X_train_prod,Y_train_prod,epochs=num_epochs,validation_data=(X_test_prod,Y_test_prod))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prod_model.save(\"product_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2638, 16)\n",
      "(2638, 16)\n"
     ]
    }
   ],
   "source": [
    "# e1 = model.layers[0]\n",
    "# weights1 = e1.get_weights()[0]\n",
    "# print(weights1.shape)\n",
    "# e2 = prod_model.layers[0]\n",
    "# weights2 = e2.get_weights()[0]\n",
    "# print(weights2.shape)\n",
    "\n",
    "# reverse_word_index1 = dict([(value,key) for (key,value) in word_index.items()])\n",
    "# reverse_word_index2 = dict([(value,key) for (key,value) in word_index_k.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_v = io.open('vecs1.tsv','w',encoding='utf-8')\n",
    "# out_m = io.open('meta1.tsv','w',encoding='utf-8')\n",
    "\n",
    "# for word_num in range(1,weights1.shape[0]):\n",
    "#     word = reverse_word_index1[word_num]\n",
    "#     embeddings = weights1[word_num]\n",
    "#     out_m.write(word + '\\n')\n",
    "#     out_v.write('\\t'.join([str(x) for x in embeddings]) + '\\n')\n",
    "# out_v.close()\n",
    "# out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_v2 = io.open('vecs2.tsv','w',encoding='utf-8')\n",
    "# out_m2 = io.open('meta2.tsv','w',encoding='utf-8')\n",
    "\n",
    "# for word_num in range(1,weights2.shape[0]):\n",
    "#     word = reverse_word_index2[word_num]\n",
    "#     embeddings = weights2[word_num]\n",
    "#     out_m2.write(word + '\\n')\n",
    "#     out_v2.write('\\t'.join([str(x) for x in embeddings]) + '\\n')\n",
    "# out_v2.close()\n",
    "# out_m2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(history,string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()\n",
    "plot_graph(history, \"acc\")\n",
    "plot_graph(history, \"loss\")\n",
    "\n",
    "plot_graph(prod_history, \"acc\")\n",
    "plot_graph(prod_history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
