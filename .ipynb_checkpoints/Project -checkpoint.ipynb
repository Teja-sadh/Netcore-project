{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Embedding, Activation, Flatten,Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto(log_device_placement = True)\n",
    "sess = tf.Session(config=config)\n",
    "tf.keras.backend.set_session(sess)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('dataset.xlsx')\n",
    "sentences = []\n",
    "labels = []\n",
    "for row in range(len(data)):\n",
    "    sentences.append(str(data['Subject'][row]))\n",
    "    labels.append(str(data['Category'][row]))\n",
    "labels = [cat.lower() for cat in labels]\n",
    "sentences = [word.lower() for word in sentences]\n",
    "X = np.array(sentences)\n",
    "Y = np.array(labels)\n",
    "X, Y = shuffle(X,Y, random_state=0)\n",
    "X_train = X[0:12000]\n",
    "X_test = X[12000:14814]\n",
    "\n",
    "#reshape Y\n",
    "Y = Y.reshape(Y.shape[0],1)\n",
    "Y_train = Y[0:12000,:]\n",
    "Y_test = Y[12000:14814,:]\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n0123456789',oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train,padding='post')\n",
    "vocab_size = len(word_index)+1\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test,padding='post',maxlen=len(X_train[1]))\n",
    "label_encoder  = LabelEncoder()\n",
    "Y_train = label_encoder.fit_transform(Y_train)\n",
    "Y_train = to_categorical(Y_train,num_classes=5)\n",
    "Y_test = label_encoder.fit_transform(Y_test)\n",
    "Y_test = to_categorical(Y_test,num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer_category.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_model(X_train):\n",
    "    max_len = len(X_train[0])\n",
    "    embedding_dim = 16\n",
    "    vocab_size = len(word_index) + 1 \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,embedding_dim,input_length=max_len))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    #model.add(Flatten())\n",
    "    model.add(Dense(16,activation = 'relu'))\n",
    "    model.add(Dense(5,activation='softmax'))\n",
    "    #model.add(Dense(max_len//2,activation = 'sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(history,string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\final_year_project\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From D:\\Anaconda\\envs\\final_year_project\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 24, 16)            42208     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 42,565\n",
      "Trainable params: 42,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 12000 samples, validate on 2814 samples\n",
      "Epoch 1/15\n",
      "12000/12000 [==============================] - 10s 802us/sample - loss: 1.3412 - acc: 0.4515 - val_loss: 1.1519 - val_acc: 0.4872\n",
      "Epoch 2/15\n",
      "12000/12000 [==============================] - 6s 532us/sample - loss: 0.8867 - acc: 0.6909 - val_loss: 0.6756 - val_acc: 0.7761\n",
      "Epoch 3/15\n",
      "12000/12000 [==============================] - 5s 457us/sample - loss: 0.5382 - acc: 0.8303 - val_loss: 0.4727 - val_acc: 0.8486\n",
      "Epoch 4/15\n",
      "12000/12000 [==============================] - 5s 421us/sample - loss: 0.3979 - acc: 0.8753 - val_loss: 0.3867 - val_acc: 0.8788\n",
      "Epoch 5/15\n",
      "12000/12000 [==============================] - 6s 527us/sample - loss: 0.3261 - acc: 0.8950 - val_loss: 0.3428 - val_acc: 0.8881\n",
      "Epoch 6/15\n",
      "12000/12000 [==============================] - 6s 521us/sample - loss: 0.2810 - acc: 0.9068 - val_loss: 0.3188 - val_acc: 0.8884\n",
      "Epoch 7/15\n",
      "12000/12000 [==============================] - 6s 484us/sample - loss: 0.2490 - acc: 0.9185 - val_loss: 0.2899 - val_acc: 0.9058\n",
      "Epoch 8/15\n",
      "12000/12000 [==============================] - 6s 525us/sample - loss: 0.2257 - acc: 0.9241 - val_loss: 0.2794 - val_acc: 0.9090\n",
      "Epoch 9/15\n",
      "12000/12000 [==============================] - 6s 520us/sample - loss: 0.2078 - acc: 0.9295 - val_loss: 0.2655 - val_acc: 0.9161\n",
      "Epoch 10/15\n",
      "12000/12000 [==============================] - 6s 522us/sample - loss: 0.1923 - acc: 0.9341 - val_loss: 0.2578 - val_acc: 0.9176\n",
      "Epoch 11/15\n",
      "12000/12000 [==============================] - 5s 385us/sample - loss: 0.1814 - acc: 0.9376 - val_loss: 0.2546 - val_acc: 0.9179\n",
      "Epoch 12/15\n",
      "12000/12000 [==============================] - 6s 522us/sample - loss: 0.1706 - acc: 0.9393 - val_loss: 0.2479 - val_acc: 0.9240\n",
      "Epoch 13/15\n",
      "12000/12000 [==============================] - 6s 520us/sample - loss: 0.1625 - acc: 0.9424 - val_loss: 0.2492 - val_acc: 0.9247\n",
      "Epoch 14/15\n",
      "12000/12000 [==============================] - 5s 392us/sample - loss: 0.1553 - acc: 0.9460 - val_loss: 0.2409 - val_acc: 0.9257\n",
      "Epoch 15/15\n",
      "12000/12000 [==============================] - 6s 523us/sample - loss: 0.1493 - acc: 0.9464 - val_loss: 0.2451 - val_acc: 0.9247\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "model = keras_model(X_train)\n",
    "history=model.fit(X_train,Y_train,epochs=num_epochs,validation_data=(X_test,Y_test))\n",
    "#plot_graph(history, \"acc\")\n",
    "#plot_graph(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('category_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {0:\"Active Again\",1:\"Easy to Bank\",2:\"Onboarding\",3:\"Regulatory & Mandatory\",4:\"Take More\"}\n",
    "products = data['Product']\n",
    "products = [i.lower() for i in products]\n",
    "X_prod = np.array(sentences)\n",
    "Y_prod = np.array(products)\n",
    "X_prod, Y_prod = shuffle(X_prod,Y_prod, random_state=0)\n",
    "X_train_prod = X_prod[0:12000]\n",
    "X_test_prod = X_prod[12000:14814]\n",
    "\n",
    "#reshape Y\n",
    "Y_prod = Y_prod.reshape(Y_prod.shape[0],1)\n",
    "Y_train_prod = Y_prod[0:12000,:]\n",
    "Y_test_prod = Y_prod[12000:14814,:]\n",
    "tokenizer_k = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n0123456789',oov_token=\"<OOV>\")\n",
    "tokenizer_k.fit_on_texts(X_train_prod)\n",
    "word_index_k = tokenizer_k.word_index\n",
    "X_train_prod = tokenizer_k.texts_to_sequences(X_train_prod)\n",
    "X_train_prod = pad_sequences(X_train_prod,padding='post')\n",
    "vocab_size_k = len(word_index_k)+1\n",
    "\n",
    "X_test_prod = tokenizer_k.texts_to_sequences(X_test_prod)\n",
    "X_test_prod = pad_sequences(X_test_prod,padding='post',maxlen=len(X_train_prod[1]))\n",
    "label_encoder  = LabelEncoder()\n",
    "Y_train_prod = label_encoder.fit_transform(Y_train_prod)\n",
    "Y_train_prod = to_categorical(Y_train_prod,num_classes=8)\n",
    "Y_test_prod = label_encoder.fit_transform(Y_test_prod)\n",
    "Y_test_prod = to_categorical(Y_test_prod,num_classes=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer_product.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer_k, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_prod_model(X_train_prod,vocab_size):\n",
    "    max_len = len(X_train_prod[0])\n",
    "    embedding_dim = 16\n",
    "    #vocab_size = len(word_index_k) + 1 \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,embedding_dim,input_length=max_len))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    #model.add(Flatten())\n",
    "    model.add(Dense(16,activation = 'relu'))\n",
    "    model.add(Dense(8,activation='softmax'))\n",
    "    #model.add(Dense(max_len//2,activation = 'sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 24, 16)            42208     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "=================================================================\n",
      "Total params: 42,616\n",
      "Trainable params: 42,616\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 12000 samples, validate on 2814 samples\n",
      "Epoch 1/15\n",
      "12000/12000 [==============================] - 5s 396us/sample - loss: 1.4337 - acc: 0.5549 - val_loss: 0.9919 - val_acc: 0.7118\n",
      "Epoch 2/15\n",
      "12000/12000 [==============================] - 4s 310us/sample - loss: 0.7981 - acc: 0.7329 - val_loss: 0.5965 - val_acc: 0.8063\n",
      "Epoch 3/15\n",
      "12000/12000 [==============================] - 6s 475us/sample - loss: 0.5294 - acc: 0.8281 - val_loss: 0.4317 - val_acc: 0.8543\n",
      "Epoch 4/15\n",
      "12000/12000 [==============================] - 6s 472us/sample - loss: 0.3945 - acc: 0.8858 - val_loss: 0.3462 - val_acc: 0.8934\n",
      "Epoch 5/15\n",
      "12000/12000 [==============================] - 6s 466us/sample - loss: 0.3124 - acc: 0.9080 - val_loss: 0.2909 - val_acc: 0.9161\n",
      "Epoch 6/15\n",
      "12000/12000 [==============================] - 4s 328us/sample - loss: 0.2517 - acc: 0.9317 - val_loss: 0.2478 - val_acc: 0.9382\n",
      "Epoch 7/15\n",
      "12000/12000 [==============================] - 4s 313us/sample - loss: 0.2037 - acc: 0.9498 - val_loss: 0.2158 - val_acc: 0.9446\n",
      "Epoch 8/15\n",
      "12000/12000 [==============================] - 4s 354us/sample - loss: 0.1687 - acc: 0.9576 - val_loss: 0.1928 - val_acc: 0.9520\n",
      "Epoch 9/15\n",
      "12000/12000 [==============================] - 4s 349us/sample - loss: 0.1429 - acc: 0.9626 - val_loss: 0.1826 - val_acc: 0.9538\n",
      "Epoch 10/15\n",
      "12000/12000 [==============================] - 4s 327us/sample - loss: 0.1244 - acc: 0.9669 - val_loss: 0.1648 - val_acc: 0.9559\n",
      "Epoch 11/15\n",
      "12000/12000 [==============================] - 4s 300us/sample - loss: 0.1095 - acc: 0.9696 - val_loss: 0.1583 - val_acc: 0.9566\n",
      "Epoch 12/15\n",
      "12000/12000 [==============================] - 3s 276us/sample - loss: 0.0978 - acc: 0.9714 - val_loss: 0.1506 - val_acc: 0.9613\n",
      "Epoch 13/15\n",
      "12000/12000 [==============================] - 4s 312us/sample - loss: 0.0880 - acc: 0.9749 - val_loss: 0.1461 - val_acc: 0.9613\n",
      "Epoch 14/15\n",
      "12000/12000 [==============================] - 4s 308us/sample - loss: 0.0806 - acc: 0.9767 - val_loss: 0.1457 - val_acc: 0.9659\n",
      "Epoch 15/15\n",
      "12000/12000 [==============================] - 6s 486us/sample - loss: 0.0734 - acc: 0.9789 - val_loss: 0.1376 - val_acc: 0.9652\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "prod_model = keras_prod_model(X_train_prod,vocab_size_k)\n",
    "prod_history=prod_model.fit(X_train_prod,Y_train_prod,epochs=num_epochs,validation_data=(X_test_prod,Y_test_prod))\n",
    "#plot_graph(prod_history, \"acc\")\n",
    "#plot_graph(prod_history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_model.save(\"product_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
